<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
    <title>mlstatpy</title>
    <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/main_0000.html</link>
    <description>blog associated to mlstatpy</description>
    
<item>
            <title>Les maths, ça bugge moins quand même</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2019/2019-05-05_maths.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2019/2019-05-05_maths.html</guid>
            <description>Trouver un bug dans un millier de lignes de codes,
c'est rarement le jeu qui apporte le plus de joie
excepté peut-être le moment l'erreur surgit sur l'autel
comme la mariée apparaît dans l'église. Les bugs font
souvent de mauvais mariages et de très bons divorces.
Le pire survient après avoir découvert qu'ils se sont
de nouveau invités dans le pâté et le fromage.
Je me suis amusé avec les régressions linéaires
:ref:`l-reglin-variations`, quantiles et par morceaux.
Et je me suis retrouvé un jour avec une question
existencielle à propos d'une régression logistique
qui ressemblait visuellement beaucoup à un diagramme
de Voronoï tant est si bien que je me suis demandé
s'ils étaient jumeaux ou simplement parent
(:ref:`l-lrvor-connection`). Je recycle quelques vieilles
idées qui m'ont ramené au temps que j'ai passé chez Yahoo
:ref:`l-graph_distance`. Et celui-là aussi
:ref:`l-k-algo-gest` dont je trouve l'idée toujours
aussi séduisante. J'ai dû fixer quelques erreurs
dans :ref:`l-roc-theoritically` car, j'ai beau faire,
je n'arrive toujours pas à retenir la définition de
*False Positive Rate*... C'est quand le prédicteur
dit blanc alors que c'est noir ou l'inverse.
Bref, je n'insiste plus, je suis un dyslexique
du classifieur. Je me suis amusé dans
d'autres domaines : :epkg:`Predictable t-SNE`,
:epkg:`Visualize a scikit-learn pipeline`,
:epkg:`Regression with confidence interval`.

Il est 1h30 du matin et je viens de trouver mon bug
de ce soir... Un bug mathématique pour changer, un
oubli dont je ne suis pas fier qui m'a fait relire mon
code encore et encore jusqu'à trouver le petit détail
qui a fait dérailler mon intuition, mais pas complètement
dérailler. Bref j'ai fini par écrire un algorithme
de streaming pour une orth-normalisation de Gram-Schmidt :
:func:`streaming_gram_schmidt_update
&lt;mlstatpy.ml.matrices.streaming_gram_schmidt_update&gt;`.
Il ne reste plus qu'à écrire un algorithme de streaming
pour la régression linéaire.</description>
            <pubDate>2019-05-05</pubDate>
        </item>
<item>
            <title>One Hundred Probability/Statistics Inequalities</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2018/2018-09-10_stat.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2018/2018-09-10_stat.html</guid>
            <description>Découvert dans un tweet :
`One Hundred Probability/Statistics Inequalities &lt;http://www.npslagle.info/articles/onehundredprobabilityinequalities.pdf&gt;`_.</description>
            <pubDate>2018-08-10</pubDate>
        </item>
<item>
            <title>Adam</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2017/2017-02-16_gradient.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2017/2017-02-16_gradient.html</guid>
            <description>`Adam &lt;https://en.wikipedia.org/wiki/Adam_(Buffy_the_Vampire_Slayer)&gt;`_
n'est pas le personnage de la saison 4
de `Buffy contre les vampires &lt;https://en.wikipedia.org/wiki/Buffy_the_Vampire_Slayer&gt;`_
mais un algorithme de descente de gradient :
`Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.
Si vous ne me croyez pas, vous devriez lire cette petite revue
`An overview of gradient descent optimization algorithms &lt;http://sebastianruder.com/optimizing-gradient-descent/&gt;`_.
Un autre algorithme intéressant est
`Hogwild &lt;http://sebastianruder.com/optimizing-gradient-descent/index.html#hogwild&gt;`_,
asynchrone et distribué. Bref, a unicorn comme disent les anglais.</description>
            <pubDate>2017-02-16</pubDate>
        </item>
<item>
            <title>Articles autour du gradient</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-08-17_gradient.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-08-17_gradient.html</guid>
            <description>* `DSA: Decentralized Double Stochastic Averaging Gradient Algorithm &lt;http://www.jmlr.org/papers/volume17/15-292/15-292.pdf&gt;`_
* `Distributed Coordinate Descent Method for Learning with Big Data &lt;http://www.jmlr.org/papers/volume17/15-001/15-001.pdf&gt;`_</description>
            <pubDate>2016-08-17</pubDate>
        </item>
<item>
            <title>Premier blog, juste un essai</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-06-19_first_blog.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-06-19_first_blog.html</guid>
            <description>Premier blog.</description>
            <pubDate>2016-06-19</pubDate>
        </item>
<item>
            <title>Lectures</title>
            <link>http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-08-09_cnn.html</link>
            <guid isPermaLink="true">http://www.xavierdupre.fr/app/mlstatpy/helpsphinx//blog/2016/2016-08-09_cnn.html</guid>
            <description>Un article intéressant plus pratique que théorique :
`Recent Advances in Convolutional Neural Networks &lt;http://arxiv.org/abs/1512.07108&gt;`_.</description>
            <pubDate>2016-06-19</pubDate>
        </item>

</channel>
</rss>
